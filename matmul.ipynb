{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE HUNT FOR THE BETTER DEEP LEARNING LIBRARY IN JULIA BEGINS...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our search to make a good deeplearning library in Julia. This search is inspired by Jeremy Howard's course \"Deep Learning from foundations\" on FastAI. Our primary motive is to understand nuts and bolts of neural networks. So, lets dive in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. THE BETTER MATRIX MULTIPLICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix Multiplication is the backbone of all neural networks. Basically every layer of neural network is a column of weight matrix. So the entire forward pass will be a matrix multiplication of weight matrix with the input matrix. So we will emulate a MNIST dataset. It'll be basically *28x28* matrix. So the input matrix is with *n* rows and *784* columns. And we have to predict one out of *10*  categories. So the weight matrix will be *784x10* matrix so that the output will be a matrix of dimensions *nx10*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rand(5, 784);\n",
    "y = rand(784,10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we need a metric to compare our different methods of matrix multiply. So lets import the \"BenchmarkTools\" package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also lets create tests for our desired output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_for_size (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function test_for_size(prod)\n",
    "    @assert size(prod) == (5,10)\n",
    "    end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: The old school for loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its said that Julia is as fast as C language. Which means the for loops will run blazingly fast. Let's see how it fares..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matmul_for_loop (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function matmul_for_loop(a,b)\n",
    "    ar, ac = size(a)\n",
    "    br, bc = size(b)\n",
    "    c = zeros(ar,bc)\n",
    "    @assert ac == br\n",
    "    for i in 1:ar\n",
    "        for j in 1:bc\n",
    "            for k in 1:ac\n",
    "                c[i,j] += a[i,k] * b[k,j];\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return c\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, let's see how fast it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  113.670 μs (1 allocation: 496 bytes)\n"
     ]
    }
   ],
   "source": [
    "@btime matmul_for_loop(x,y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woahhhhh.... 113 microseconds. Its way faster than Julia's counterpart Python. But I guess we can do better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_for_size(matmul_for_loop(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But our test passed..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Multithreading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take advantages of 4 cores of my cpu and Julia's parallel processing capabilities. First lets create a julia kernel with 4 threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Installing Julia 4 Threads kernelspec in /home/vishal/.local/share/jupyter/kernels/julia-4-threads-1.5\n",
      "└ @ IJulia /home/vishal/.julia/packages/IJulia/DrVMH/deps/kspec.jl:78\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"/home/vishal/.local/share/jupyter/kernels/julia-4-threads-1.5\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using IJulia\n",
    "IJulia.installkernel(\"Julia 4 Threads\", env=Dict(\n",
    "    \"JULIA_NUM_THREADS\" => \"4\",\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets change our kernel to 4 threaded kernel. This can be done by Kernel tab -->  Change Kernel and changing the kernel to \"Julia 4 Threads (version no.)\". Don't forget to initialise the vital stuff after changing the kernel. Now lets slightly modify our matmul function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matmul_for_loop_threads (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function matmul_for_loop_threads(a,b)\n",
    "    ar, ac = size(a)\n",
    "    br, bc = size(b)\n",
    "    c = zeros(ar,bc)\n",
    "    @assert ac == br\n",
    "    Threads.@threads for i in 1:ar\n",
    "        Threads.@threads for j in 1:bc\n",
    "            Threads.@threads for k in 1:ac\n",
    "                c[i,j] += a[i,k] * b[k,j];\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return c\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_for_size(matmul_for_loop_threads(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  76.333 μs (131 allocations: 7.77 KiB)\n"
     ]
    }
   ],
   "source": [
    "@btime matmul_for_loop_threads(x,y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used lot of allocations and memory but we have done almost twice faster. Kudos...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Julia's usual matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The good thing is that julia follows  Matlab in most of ways. One of it is it can implicitly multiply matrices like multiplying any other datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_for_size(x*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9.497 μs (1 allocation: 496 bytes)\n"
     ]
    }
   ],
   "source": [
    "@btime x*y;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damnnnn.... This is the best we can get. The raw power of julia. For comparison, the best deeplearning library in python, pytorch takes 18 µs to do this exact same matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 4: Using the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already we have a huge margin against the vendor libraries in python. Lets push a little bit further. Lets do it on our GPU. For that let's use ArrayFire. There are many libraries to do GPU computation in Julia. Infact the last thing Julia needs is another GPU Library. But I choose CUDA.jl which is written in pure Julia, because to keep our library as pure as possible, so that the users don't need to learn some other language to do some minor modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = CuArray(x);\n",
    "Y = CuArray(y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_for_size(X*Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3.168 μs (4 allocations: 288 bytes)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5×10 CuArray{Float64,2}:\n",
       " 193.154  190.949  187.937  184.728  …  192.117  196.327  190.054  203.687\n",
       " 198.127  193.826  193.748  189.603     197.958  195.956  190.37   209.801\n",
       " 195.861  193.241  195.062  191.529     197.368  202.52   195.356  201.014\n",
       " 190.758  192.196  191.103  188.756     192.836  193.453  190.691  206.937\n",
       " 188.686  187.003  188.603  182.944     183.496  192.013  184.795  193.556"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@btime X*Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is lightning fast. Though I have very poor gpu (920MX) it accelerated our code thrice faster. Imagine what kind of power it'll have in some good gpus like 1080Ti, 2080Ti. We will take our "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 4 Threads 1.5.0",
   "language": "julia",
   "name": "julia-4-threads-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
